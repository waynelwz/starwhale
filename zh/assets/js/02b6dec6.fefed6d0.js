"use strict";(self.webpackChunkstarwhale_docs=self.webpackChunkstarwhale_docs||[]).push([[482],{3905:function(e,t,n){n.d(t,{Zo:function(){return c},kt:function(){return u}});var a=n(7294);function l(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function o(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function i(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?o(Object(n),!0).forEach((function(t){l(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):o(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,a,l=function(e,t){if(null==e)return{};var n,a,l={},o=Object.keys(e);for(a=0;a<o.length;a++)n=o[a],t.indexOf(n)>=0||(l[n]=e[n]);return l}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(a=0;a<o.length;a++)n=o[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(l[n]=e[n])}return l}var r=a.createContext({}),d=function(e){var t=a.useContext(r),n=t;return e&&(n="function"==typeof e?e(t):i(i({},t),e)),n},c=function(e){var t=d(e.components);return a.createElement(r.Provider,{value:t},e.children)},m={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},p=a.forwardRef((function(e,t){var n=e.components,l=e.mdxType,o=e.originalType,r=e.parentName,c=s(e,["components","mdxType","originalType","parentName"]),p=d(n),u=l,h=p["".concat(r,".").concat(u)]||p[u]||m[u]||o;return n?a.createElement(h,i(i({ref:t},c),{},{components:n})):a.createElement(h,i({ref:t},c))}));function u(e,t){var n=arguments,l=t&&t.mdxType;if("string"==typeof e||l){var o=n.length,i=new Array(o);i[0]=p;var s={};for(var r in t)hasOwnProperty.call(t,r)&&(s[r]=t[r]);s.originalType=e,s.mdxType="string"==typeof e?e:l,i[1]=s;for(var d=2;d<o;d++)i[d]=n[d];return a.createElement.apply(null,i)}return a.createElement.apply(null,n)}p.displayName="MDXCreateElement"},9777:function(e,t,n){n.r(t),n.d(t,{assets:function(){return c},contentTitle:function(){return r},default:function(){return u},frontMatter:function(){return s},metadata:function(){return d},toc:function(){return m}});var a=n(7462),l=n(3366),o=(n(7294),n(3905)),i=["components"],s={title:"Audio Classification on Speech Commands dataset"},r=void 0,d={unversionedId:"tutorials/speech",id:"tutorials/speech",title:"Audio Classification on Speech Commands dataset",description:"This example illustrates how to evaluate a pre-trained audio classification model on Starwhale(version:0.2.0b8) in 6 steps.",source:"@site/docs/tutorials/speech.md",sourceDirName:"tutorials",slug:"/tutorials/speech",permalink:"/zh/docs/tutorials/speech",draft:!1,editUrl:"https://github.com/star-whale/starwhale/tree/main/docs/docs/tutorials/speech.md",tags:[],version:"current",frontMatter:{title:"Audio Classification on Speech Commands dataset"},sidebar:"mainSidebar",previous:{title:"Object Detection & segmentation on PeenFudanPed dataset",permalink:"/zh/docs/tutorials/pfp"},next:{title:"NMT for french and engish",permalink:"/zh/docs/tutorials/nmt"}},c={},m=[{value:"Prerequisites",id:"prerequisites",level:2},{value:"Create a Runtime",id:"create-a-runtime",level:2},{value:"Train the model",id:"train-the-model",level:2},{value:"Slice the test dataset using the Starwhale protocol",id:"slice-the-test-dataset-using-the-starwhale-protocol",level:2},{value:"Implement the inference method and evaluation metrics computing method",id:"implement-the-inference-method-and-evaluation-metrics-computing-method",level:2},{value:"Implement ppl",id:"implement-ppl",level:3},{value:"Implement cmp",id:"implement-cmp",level:3},{value:"Build Runtime, Model, and Dataset",id:"build-runtime-model-and-dataset",level:2},{value:"Build Runtime",id:"build-runtime",level:3},{value:"Build Dataset",id:"build-dataset",level:3},{value:"Build Model",id:"build-model",level:3},{value:"Run the evaluation job and see the metrics",id:"run-the-evaluation-job-and-see-the-metrics",level:2},{value:"Evaluate the model on the local standalone instance",id:"evaluate-the-model-on-the-local-standalone-instance",level:3},{value:"Create a job",id:"create-a-job",level:4},{value:"See the metrics",id:"see-the-metrics",level:4},{value:"Evaluate the model on a cloud instance",id:"evaluate-the-model-on-a-cloud-instance",level:3}],p={toc:m};function u(e){var t=e.components,n=(0,l.Z)(e,i);return(0,o.kt)("wrapper",(0,a.Z)({},p,n,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("p",null,"This example illustrates how to evaluate a pre-trained audio classification model on Starwhale(",(0,o.kt)("inlineCode",{parentName:"p"},"version:0.2.0b8"),") in 6 steps."),(0,o.kt)("ol",null,(0,o.kt)("li",{parentName:"ol"},"Create a Runtime"),(0,o.kt)("li",{parentName:"ol"},"Train the model"),(0,o.kt)("li",{parentName:"ol"},"Implement the dataset slicing method"),(0,o.kt)("li",{parentName:"ol"},"Implement the inference method and evaluation metrics computing method"),(0,o.kt)("li",{parentName:"ol"},"Build Runtime, Model, and Dataset"),(0,o.kt)("li",{parentName:"ol"},"Run the evaluation job and see the metrics")),(0,o.kt)("h2",{id:"prerequisites"},"Prerequisites"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"Assume that you have Python3.7 or above installed."),(0,o.kt)("li",{parentName:"ul"},"Clone starwhale repo")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-shell"},"git clone https://github.com/star-whale/starwhale.git\ncd starwhale/example/speech_command\n")),(0,o.kt)("blockquote",null,(0,o.kt)("p",{parentName:"blockquote"},"\ud83d\udca1 If you are from the mainland of China, we strongly recommend you use a proxy.")),(0,o.kt)("h2",{id:"create-a-runtime"},"Create a Runtime"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-shell"},"$ swcli runtime create . --name audio_pytorch -m venv --python=3.8 --force\n\ud83d\udea7 start to create runtime environment...\n\ud83d\udc4f create venv@~/code/starwhale/example/speech_command/venv, python:3.8.10 (default, Mar 15 2022, 12:22:08)\n[GCC 9.4.0]\n\ud83d\udc36 install starwhale==0.2.0b8 venv@~/code/starwhale/example/speech_command/venv...\n\ud83c\udf70 run command in shell \ud83c\udf70\n        ~/code/starwhale/example/speech_command/venv/bin/activate\n\ud83d\udc4f python runtime environment is ready to use \ud83c\udf89\n$ source ~/code/starwhale/example/speech_command/venv/bin/activate\n(audio_pytorch) $  python3 -m pip install -r requirements.txt\n")),(0,o.kt)("blockquote",null,(0,o.kt)("p",{parentName:"blockquote"},"\ud83d\udca1 make sure python3.8-venv is installed if you choose --python=3.8\n\ud83d\udca1 ",(0,o.kt)("inlineCode",{parentName:"p"},"python3 -m pip install")," is recommended over ",(0,o.kt)("inlineCode",{parentName:"p"},"pip install"))),(0,o.kt)("h2",{id:"train-the-model"},"Train the model"),(0,o.kt)("blockquote",null,(0,o.kt)("p",{parentName:"blockquote"},"The training code in this repo is copied from ",(0,o.kt)("a",{parentName:"p",href:"https://pytorch.org/tutorials/intermediate/speech_command_classification_with_torchaudio_tutorial.html"},"torchaudio tutorial"),". However, some code is modified to understand better how Starwhale works.")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-shell"},"(audio_pytorch) $ mkdir models\n(audio_pytorch) $ mkdir data\n(audio_pytorch) $ cd code\n(audio_pytorch) $ python train.py\n")),(0,o.kt)("p",null,"You will get the logs as below:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-shell"},"  0%|                                                          | 0/1 [00:00<?, ?it/s]Train Epoch: 1 [0/84843 (0%)]   Loss: 3.755602\n  5%|\u2588\u2588\u2588\u258a                                                      | 0.047846889952153096/1 [00:18<06:13, 392.78s/it]Train Epoch: 1 [5120/84843 (6%)]        Loss: 3.069063\n 10%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b                                                   | 0.09569377990430618/1 [00:36<05:49, 386.05s/it]Train Epoch: 1 [10240/84843 (12%)]      Loss: 2.678599\n 14%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d                                                 | 0.14354066985645936/1 [00:53<04:53, 342.60s/it]Train Epoch: 1 [15360/84843 (18%)]      Loss: 2.131145\n 19%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e                                              | 0.19138755980861272/1 [01:10<04:33, 338.02s/it]Train Epoch: 1 [20480/84843 (24%)]      Loss: 2.012789\n 24%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                                            | 0.23923444976076608/1 [01:27<05:03, 398.31s/it]Train Epoch: 1 [25600/84843 (30%)]      Loss: 1.740537\n 29%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589                                         | 0.28708133971291944/1 [01:44<04:34, 385.32s/it]Train Epoch: 1 [30720/84843 (36%)]      Loss: 1.725273\n 33%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                                       | 0.3349282296650728/1 [02:02<04:06, 371.29s/it]Train Epoch: 1 [35840/84843 (42%)]      Loss: 1.671064\n 38%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c                                   | 0.38277511961722616/1 [02:19<03:43, 362.65s/it]Train Epoch: 1 [40960/84843 (48%)]      Loss: 1.381939\n 43%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a                                | 0.4306220095693795/1 [02:36<03:15, 342.59s/it]Train Epoch: 1 [46080/84843 (54%)]      Loss: 1.435907\n 48%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c                              | 0.4784688995215329/1 [02:55<03:25, 393.13s/it]Train Epoch: 1 [51200/84843 (60%)]      Loss: 1.576231\n 53%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d                           | 0.5263157894736856/1 [03:13<02:38, 335.18s/it]Train Epoch: 1 [56320/84843 (66%)]      Loss: 1.206882\n 57%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e                        | 0.5741626794258379/1 [03:29<02:32, 357.79s/it]Train Epoch: 1 [61440/84843 (72%)]      Loss: 1.126174\n 62%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f                     | 0.6220095693779901/1 [03:46<02:05, 331.93s/it]Train Epoch: 1 [66560/84843 (78%)]      Loss: 1.345579\n 67%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                    | 0.6698564593301424/1 [04:02<01:48, 328.73s/it]Train Epoch: 1 [71680/84843 (84%)]      Loss: 1.211446\n 72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589                | 0.7177033492822946/1 [04:18<01:33, 330.40s/it]Train Epoch: 1 [76800/84843 (90%)]      Loss: 1.104123\n 77%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a             | 0.7655502392344469/1 [04:34<01:20, 344.70s/it]Train Epoch: 1 [81920/84843 (96%)]      Loss: 1.253487\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 0.9999999999999929/1 [05:53<00:00, 353.79s/it]\nTest Epoch: 1   Accuracy: 12200/22010 (55%)\n\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 0.9999999999999929/1 [05:53<00:00, 353.91s/it]\n")),(0,o.kt)("p",null,"Great! Now, you have your model trained and saved. You can see it in the ",(0,o.kt)("inlineCode",{parentName:"p"},"models")," directory."),(0,o.kt)("h2",{id:"slice-the-test-dataset-using-the-starwhale-protocol"},"Slice the test dataset using the Starwhale protocol"),(0,o.kt)("p",null,"In the training section, we use a dataset called ",(0,o.kt)("a",{parentName:"p",href:"https://paperswithcode.com/dataset/speech-commands"},"SpeechCommands"),"."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-shell"},"(audio_pytorch) $ ls ../data\nSpeechCommands  speech_commands_v0.02.tar.gz\n")),(0,o.kt)("p",null,"Before version ",(0,o.kt)("inlineCode",{parentName:"p"},"0.2.x"),", Starwhale sliced the dataset into chunks where the batched audios and labels reside. You must tell Starwhale how to yield batches of byte arrays from each dataset file."),(0,o.kt)("p",null,"To read all test files in this dataset, we overwrite ",(0,o.kt)("inlineCode",{parentName:"p"},"load_list")," method of the parent class ",(0,o.kt)("inlineCode",{parentName:"p"},"BuildExecutor")," in Starwhale SDK.\nTo package audios and labels in batches and convert them into byte arrays, we overwrite ",(0,o.kt)("inlineCode",{parentName:"p"},"iter_all_dataset_slice")," and ",(0,o.kt)("inlineCode",{parentName:"p"},"iter_all_label_slice")," methods of the parent class ",(0,o.kt)("inlineCode",{parentName:"p"},"BuildExecutor")," in Starwhale SDK. We package paths of audios into ",(0,o.kt)("inlineCode",{parentName:"p"},"FileBytes")," so that it is easier to debug."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'class FileBytes:\n    def __init__(self, p):\n        self.file_path = p\n        self.content_bytes = open(p, "rb").read()\n\n\ndef _pickle_data(audio_file_paths):\n    all_bytes = [FileBytes(audio_f) for audio_f in audio_file_paths]\n    return pickle.dumps(all_bytes)\n\n\ndef _pickle_label(audio_file_paths):\n    all_strings = [os.path.basename(os.path.dirname(str(audio_f))) for audio_f\n                   in audio_file_paths]\n    return pickle.dumps(all_strings)\n\n\nclass SpeechCommandsSlicer(BuildExecutor):\n\n    def load_list(self, file_filter):\n        filepath = self.data_dir / file_filter\n        with open(filepath) as fileobj:\n            return [self.data_dir / line.strip() for line in fileobj]\n\n    def _iter_files(\n        self, file_filter: str, sort_key: t.Optional[t.Any] = None\n    ) -> t.Generator[Path, None, None]:\n        _key = sort_key\n        if _key is not None and not callable(_key):\n            raise Exception(f"data_sort_func({_key}) is not callable.")\n\n        _files = sorted(self.load_list(file_filter), key=_key)\n        for p in _files:\n            if not p.is_file():\n                continue\n            yield p\n\n    def iter_data_slice(self, path: str):\n        pass\n\n    def iter_label_slice(self, path: str):\n        pass\n\n    def iter_all_dataset_slice(self) -> t.Generator[t.Any, None, None]:\n        datafiles = [p for p in self.iter_data_files()]\n        idx = 0\n        data_size = len(datafiles)\n        while True:\n            last_idx = idx\n            idx += 1\n            if idx > data_size:\n                break\n            yield _pickle_data(datafiles[last_idx:idx])\n\n    def iter_all_label_slice(self) -> t.Generator[t.Any, None, None]:\n        datafiles = [p for p in self.iter_data_files()]\n        idx = 0\n        data_size = len(datafiles)\n        while True:\n            last_idx = idx\n            idx += 1\n            if idx > data_size:\n                break\n            yield _pickle_label(datafiles[last_idx:idx])\n')),(0,o.kt)("p",null,"You need to extend the abstract class ",(0,o.kt)("inlineCode",{parentName:"p"},"BuildExecutor"),", so Starwhale can use it."),(0,o.kt)("h2",{id:"implement-the-inference-method-and-evaluation-metrics-computing-method"},"Implement the inference method and evaluation metrics computing method"),(0,o.kt)("p",null,"The inference method is called ",(0,o.kt)("inlineCode",{parentName:"p"},"ppl"),", and the evaluation metrics computing method is called ",(0,o.kt)("inlineCode",{parentName:"p"},"cmp"),".\nHere is the code snap from ",(0,o.kt)("inlineCode",{parentName:"p"},"ppl.py"),", which implements both methods. You need to extend the abstract class ",(0,o.kt)("inlineCode",{parentName:"p"},"PipelineHandler")," so you can receive the byte arrays, which you transformed in the last step."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'class M5Inference(PipelineHandler):\n\n    def __init__(self, device="cpu") -> None:\n        super().__init__(merge_label=True, ignore_error=True)\n        self.device = torch.device(device)\n        self.model = self._load_model(self.device)\n        self.transform = torchaudio.transforms.Resample(orig_freq=16000,\n                                                        new_freq=8000)\n        self.transform = self.transform.to(device)\n\n    def ppl(self, data, **kw):\n        audios = self._pre(data)\n        result = []\n        for audio_f in audios:\n            try:\n                label_idx = self.model(audio_f.unsqueeze(0)).argmax(\n                    dim=-1).squeeze()\n                result.append(labels[label_idx])\n            except Exception:\n                result.append(\'ERROR\')\n        return result, None\n\n    def handle_label(self, label, **kw):\n        return pickle.loads(label)\n\n    @multi_classification(\n        confusion_matrix_normalize="all",\n        show_hamming_loss=True,\n        show_cohen_kappa_score=True,\n        show_roc_auc=False,\n        all_labels=labels,\n    )\n    def cmp(self, _data_loader):\n        _result, _label, _pr = [], [], []\n        for _data in _data_loader:\n            _label.extend(_data["label"])\n            _result.extend(_data["result"])\n            # _pr.extend(_data["pr"])\n        return _result, _label\n\n    def _pre(self, input: bytes):\n        audios = pickle.loads(input)\n        _result = []\n        for file_bytes in audios:\n            # you could debug the file name by watching file_bytes.file_path\n            bytes_io = io.BytesIO(file_bytes.content_bytes)\n            test_tensor = torchaudio.load(bytes_io)[0].to(self.device)\n            test_tensor = self.transform(test_tensor)\n            _result.append(test_tensor.to(self.device))\n        return _result\n\n    def _post(self, input):\n        pred_value = input.argmax(1).flatten().tolist()\n        probability_matrix = np.exp(input.tolist()).tolist()\n        return pred_value, probability_matrix\n\n    def _load_model(self, device):\n        model = M5(n_input=1, n_output=35)\n        model.load_state_dict(torch.load(str(ROOTDIR / "models/m5.pth")))\n        model.to(device)\n        model.eval()\n        print("m5 model loaded, start to inference...")\n        return model\n')),(0,o.kt)("h3",{id:"implement-ppl"},"Implement ppl"),(0,o.kt)("p",null,"Starwhale will feed the byte arrays of one batch to the ",(0,o.kt)("inlineCode",{parentName:"p"},"ppl")," method and put  the output of ",(0,o.kt)("inlineCode",{parentName:"p"},"ppl")," into an ",(0,o.kt)("inlineCode",{parentName:"p"},"inference_result")," dict, which looks like"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-json"},'{"result":[{resultObj1},{resultObj2}],"label":[{labelObj1},{labelObj2}]}\n')),(0,o.kt)("p",null,"Starwhale will automatically add the result of ",(0,o.kt)("inlineCode",{parentName:"p"},"ppl")," to ",(0,o.kt)("inlineCode",{parentName:"p"},"inference_result.result")," and the result of ",(0,o.kt)("inlineCode",{parentName:"p"},"handle_label")," to ",(0,o.kt)("inlineCode",{parentName:"p"},"inference_result.label"),"."),(0,o.kt)("p",null,"The ",(0,o.kt)("inlineCode",{parentName:"p"},"inference_result")," is used in the argument of ",(0,o.kt)("inlineCode",{parentName:"p"},"cmp")," named ",(0,o.kt)("inlineCode",{parentName:"p"},"_data_loader"),"."),(0,o.kt)("h3",{id:"implement-cmp"},"Implement cmp"),(0,o.kt)("p",null,(0,o.kt)("inlineCode",{parentName:"p"},"_data_loader")," is an iterator for ",(0,o.kt)("inlineCode",{parentName:"p"},"result")," and ",(0,o.kt)("inlineCode",{parentName:"p"},"label"),". For a multiple classification problem, it is pretty easy to implement the ",(0,o.kt)("inlineCode",{parentName:"p"},"cmp")," method by annotating your ",(0,o.kt)("inlineCode",{parentName:"p"},"cmp")," method with the ",(0,o.kt)("inlineCode",{parentName:"p"},"multi_classification")," annotation and coping the lines inside it."),(0,o.kt)("p",null,"If you need to show ",(0,o.kt)("inlineCode",{parentName:"p"},"roc")," and ",(0,o.kt)("inlineCode",{parentName:"p"},"auc"),", you will also need to supply ",(0,o.kt)("inlineCode",{parentName:"p"},"_pr")," in your ",(0,o.kt)("inlineCode",{parentName:"p"},"ppl")," method. By now, we have finished all the coding parts. Then let's begin the command line part."),(0,o.kt)("h2",{id:"build-runtime-model-and-dataset"},"Build Runtime, Model, and Dataset"),(0,o.kt)("h3",{id:"build-runtime"},"Build Runtime"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-shell"},"(audio_pytorch) $ cd ..\n(audio_pytorch) $ swcli runtime build .\n\ud83d\udea7 start to build runtime bundle...\n\ud83d\udc77 uri:local/project/self/runtime/audio_pytorch\n\ud83d\udc26 runtime will ignore pypi editable package\n\ud83c\udd95 version ga2wkmbwmizw\n\ud83d\udcc1 workdir: /home/anda/.cache/starwhale/self/workdir/runtime/audio_pytorch/ga/ga2wkmbwmizwkn3bmuytsmjunv3dc3q\n\ud83d\udcab python3.8.10@venv, os(Linux), include-editable(False), try to export environment...\n\ud83c\udf08 runtime docker image: ghcr.io/star-whale/starwhale:0.2.0b8  \ud83c\udf08\n\ud83e\udd8b .swrt bundle:/home/anda/.cache/starwhale/self/runtime/audio_pytorch/ga/ga2wkmbwmizwkn3bmuytsmjunv3dc3q.swrt\n  7 out of 7 steps finished \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100% 0:00:00 0:00:02\n")),(0,o.kt)("h3",{id:"build-dataset"},"Build Dataset"),(0,o.kt)("p",null,"Here is some descriptive information needed for Starwhale to build a Starwhale Dataset(SWDS). A yaml file describes the information as below:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-yaml"},'name: SpeechCommands\ndata_dir: data/SpeechCommands/speech_commands_v0.02\ndata_filter: "testing_list.txt"\nlabel_filter: "testing_list.txt"\nprocess: code.data_slicer:SpeechCommandsSlicer\ndesc: SpeechCommands data and label test dataset\ntag:\n  - bin\n\nattr:\n  alignment_size: 4k\n  volume_size: 64M\n')),(0,o.kt)("p",null,"Most of the fields are self-explained. The ",(0,o.kt)("inlineCode",{parentName:"p"},"process")," descriptor is the entry point of the data split method, and Starwhale will use the files in ",(0,o.kt)("inlineCode",{parentName:"p"},"testing_list.txt")," as the input for ",(0,o.kt)("inlineCode",{parentName:"p"},"process"),"."),(0,o.kt)("p",null,"After creating the yaml file under ",(0,o.kt)("inlineCode",{parentName:"p"},"${code_base}/example/speech_command/"),", we are ready."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-shell"},"(audio_pytorch) $ swcli dataset build .\n\ud83d\udea7 start to build dataset bundle...\n\ud83d\udc77 uri:local/project/self/dataset/speechcommands\n\ud83c\udd95 version gmzgczrqmezd\n\ud83d\udcc1 swds workdir: /home/anda/.cache/starwhale/self/dataset/speechcommands/gm/gmzgczrqmezdmmtdmq2dezbtmjqtimq.swds\n\ud83d\udc4d try to copy source code files...\n\ud83d\udde3  swcli python prefix:/usr, runtime env python prefix:/mnt/c/Users/renyanda/Documents/code/starwhale/example/speech_command/venv, swcli will inject sys.path\n\ud83d\udc7b import code.data_slicer:SpeechCommandsSlicer@/mnt/c/Users/renyanda/Documents/code/starwhale/example/speech_command to make swds...\ncleanup done.\nfinish gen swds @ /home/anda/.cache/starwhale/self/dataset/speechcommands/gm/gmzgczrqmezdmmtdmq2dezbtmjqtimq.swds/data\n\ud83e\udd16 calculate signature...\n\ud83c\udf3a congratulation! you can run  swcli dataset info speechcommands/version/gmzgczrqmezdmmtdmq2dezbtmjqtimq\n  8 out of 8 steps finished \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100% 0:00:00 0:01:54\n")),(0,o.kt)("p",null,"There is one more step left."),(0,o.kt)("h3",{id:"build-model"},"Build Model"),(0,o.kt)("p",null,"Here is some descriptive information for Starwhale to build a Starwhale Model Package(SWMP). A yaml file describes the information as below:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-yaml"},"version: 1.0\nname: m5\nmodel:\n  - models/m5.pth\nrun:\n  ppl: code.ppl:M5Inference\n\ndesc: m5 by pytorch\ntag:\n  - speech command classification\n")),(0,o.kt)("p",null,"Most of the fields are self-explained. The ",(0,o.kt)("inlineCode",{parentName:"p"},"ppl")," descriptor is the entry point of the inference and cmp method.\nAfter creating the yaml file under ",(0,o.kt)("inlineCode",{parentName:"p"},"${code_base}/example/speech_command/"),", we are ready."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-shell"},"(audio_pytorch) $ swcli model build .\n\ud83d\udea7 start to build model bundle...\n\ud83d\udc77 uri:local/project/self/model/m5\n\ud83c\udd95 version mfstoolehayd\n\ud83d\udcc1 workdir: /home/anda/.cache/starwhale/self/workdir/model/m5/mf/mfstoolehaydeyrvmyzdamzrmzshuma\n\ud83d\udc4d try to copy source code files...\n\ud83e\udd8b .swmp bundle:/home/anda/.cache/starwhale/self/model/m5/mf/mfstoolehaydeyrvmyzdamzrmzshuma.swmp\n  6 out of 6 steps finished \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100% 0:00:00 0:00:17\n")),(0,o.kt)("p",null,"Here we are. We have finished all the complex parts."),(0,o.kt)("h2",{id:"run-the-evaluation-job-and-see-the-metrics"},"Run the evaluation job and see the metrics"),(0,o.kt)("p",null,"We have two ways to evaluate our model."),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"Evaluate the model on the local standalone instance"),(0,o.kt)("li",{parentName:"ul"},"Evaluate the model on a cloud instance")),(0,o.kt)("h3",{id:"evaluate-the-model-on-the-local-standalone-instance"},"Evaluate the model on the local standalone instance"),(0,o.kt)("h4",{id:"create-a-job"},"Create a job"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-shell"},"$ swcli job create self --model m5/version/latest --dataset speechcommands/version/latest --runtime audio_pytorch/version/latest\n\ud83d\ude39 /home/anda/.cache/starwhale/self/workdir/model/m5/mf/mfstoolehaydeyrvmyzdamzrmzshuma existed, skip extract model bundle\n\ud83d\udc4f render swds speechcommands:gmzgczrqmezdmmtdmq2dezbtmjqtimq local_fuse.json\n\ud83d\udd0d /home/anda/.cache/starwhale/self/dataset/speechcommands/gm/gmzgczrqmezdmmtdmq2dezbtmjqtimq.swds/local_fuse.json\ntry to import code.ppl:M5Inference@/home/anda/.cache/starwhale/self/workdir/model/m5/mf/mfstoolehaydeyrvmyzdamzrmzshuma/src...\n\ud83d\udde3  swcli python prefix:/usr, runtime env python prefix:/mnt/c/Users/renyanda/Documents/code/starwhale/example/speech_command/venv, swcli will inject sys.path\nm5 model loaded, start to inference...\n\ud83d\udc4f finish run ppl: PipelineHandler status@/home/anda/.cache/starwhale/self/job/gu/gu2doojqmi2tmzrtgzqwinlfgq2wy3q/ppl/status, log@/home/anda/.cache/starwhale/self/job/gu/gu2doojqmi2tmzrtgzqwinlfgq2wy3q/ppl/log, result@/home/anda/.cache/starwhale/self/job/gu/gu2doojqmi2tmzrtgzqwinlfgq2wy3q/ppl/result\ntry to import code.ppl:M5Inference@/home/anda/.cache/starwhale/self/workdir/model/m5/mf/mfstoolehaydeyrvmyzdamzrmzshuma/src...\n\ud83d\udde3  swcli python prefix:/usr, runtime env python prefix:/mnt/c/Users/renyanda/Documents/code/starwhale/example/speech_command/venv, swcli will inject sys.path\nm5 model loaded, start to inference...\n/mnt/c/Users/renyanda/Documents/code/starwhale/example/speech_command/venv/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/mnt/c/Users/renyanda/Documents/code/starwhale/example/speech_command/venv/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/mnt/c/Users/renyanda/Documents/code/starwhale/example/speech_command/venv/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n\ud83d\udc4f finish run cmp: PipelineHandler status@/home/anda/.cache/starwhale/self/job/gu/gu2doojqmi2tmzrtgzqwinlfgq2wy3q/cmp/status, log@/home/anda/.cache/starwhale/self/job/gu/gu2doojqmi2tmzrtgzqwinlfgq2wy3q/cmp/log, result@/home/anda/.cache/starwhale/self/job/gu/gu2doojqmi2tmzrtgzqwinlfgq2wy3q/cmp/result\n  7 out of 7 steps finished \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100% 0:00:00 0:00:37\n\ud83d\udc4f success to create job(project id: local/project/self)\n\ud83d\udc26 run cmd to fetch job info: swcli job info gu2doojqmi2t\n")),(0,o.kt)("h4",{id:"see-the-metrics"},"See the metrics"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-shell"},"(audio_pytorch) $ swcli job info gu2doojqmi2t\n\u256d\u2500 Starwhale Instance \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502                                                     \u2b50 local (local) \ud83d\udc33                                                     \ud83e\udd21anda@normal \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Inspect _manifest.yaml for eval:local/project/self/job/gu2doojqmi2t \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n{\n    'created_at': '2022-06-21 18:53:05 CST',\n    'datasets': [\n        'local/project/self/dataset/speechcommands/version/latest'\n    ],\n    'desc': None,\n    'finished_at': '2022-06-21 18:53:43 CST',\n    'model': 'm5/version/latest',\n    'model_dir': '/home/anda/.cache/starwhale/self/workdir/model/m5/mf/mfstoolehaydeyrvmyzdamzrmzshuma/src',\n    'name': None,\n    'phase': 'all',\n    'runtime': 'audio_pytorch/version/latest',\n    'status': 'success',\n    'version': 'gu2doojqmi2tmzrtgzqwinlfgq2wy3q'\n}\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Evaluation process dirs \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\ud83c\udf35 ppl: /home/anda/.cache/starwhale/self/job/gu/gu2doojqmi2tmzrtgzqwinlfgq2wy3q/ppl\n\ud83d\udc2b cmp: /home/anda/.cache/starwhale/self/job/gu/gu2doojqmi2tmzrtgzqwinlfgq2wy3q/cmp\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 MULTI_CLASSIFICATION Report \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nSummary\n\u251c\u2500\u2500 accuracy: 0.5372                                Label      Precision   Recall   F1-score   Support\n\u251c\u2500\u2500 macro avg                                      \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\u2502   \u251c\u2500\u2500 precision: 0.5152                           backward   0.5394      0.9368   0.6846     190.0000\n\u2502   \u251c\u2500\u2500 recall: 0.6146                              bed        0.2995      0.4559   0.3615     272.0000\n\u2502   \u251c\u2500\u2500 f1-score: 0.5241                            bird       0.3622      0.4085   0.3840     328.0000\n\u2502   \u2514\u2500\u2500 support: 21760.0000                         cat        0.4948      0.4898   0.4923     392.0000\n\u251c\u2500\u2500 weighted avg                                    dog        0.3364      0.5481   0.4169     270.0000\n\u2502   \u251c\u2500\u2500 precision: 0.5927                           down       0.6478      0.6415   0.6446     820.0000\n\u2502   \u251c\u2500\u2500 recall: 0.5372                              eight      0.4926      0.7417   0.5920     542.0000\n\u2502   \u251c\u2500\u2500 f1-score: 0.5247                            five       0.7416      0.5946   0.6600     1110.0000\n\u2502   \u2514\u2500\u2500 support: 21760.0000                         follow     0.5058      0.5472   0.5257     318.0000\n\u251c\u2500\u2500 hamming_loss: 0.4628                            forward    0.3097      0.6076   0.4103     158.0000\n\u2514\u2500\u2500 cohen_kappa_score: 0.5231                       four       0.6400      0.5995   0.6191     854.0000\n                                                    go         0.2736      0.6145   0.3787     358.0000\n                                                    happy      0.6847      0.8176   0.7453     340.0000\n                                                    house      0.7749      0.2919   0.4241     1014.0000\n                                                    learn      0.3416      0.5189   0.4120     212.0000\n                                                    left       0.4539      0.7165   0.5557     522.0000\n                                                    marvin     0.5436      0.8346   0.6584     254.0000\n                                                    nine       0.5833      0.7653   0.6620     622.0000\n                                                    no         0.2568      0.8814   0.3977     236.0000\n                                                    off        0.6219      0.5910   0.6061     846.0000\n                                                    on         0.7146      0.7093   0.7119     798.0000\n                                                    one        0.5439      0.8411   0.6606     516.0000\n                                                    right      0.5354      0.7186   0.6136     590.0000\n                                                    seven      0.7438      0.5374   0.6240     1124.0000\n                                                    sheila     0.8396      0.3321   0.4759     1072.0000\n                                                    six        0.8909      0.4067   0.5585     1726.0000\n                                                    stop       0.7421      0.5360   0.6224     1138.0000\n                                                    three      0.3827      0.4282   0.4042     724.0000\n                                                    tree       0.7098      0.2171   0.3325     1262.0000\n                                                    two        0.2712      0.6647   0.3853     346.0000\n                                                    up         0.2635      0.8889   0.4065     252.0000\n                                                    visual     0.4970      0.9011   0.6406     182.0000\n                                                    wow        0.3883      0.8989   0.5424     178.0000\n                                                    yes        0.6205      0.7450   0.6771     698.0000\n                                                    zero       0.4983      0.6986   0.5817     418.0000\n                                                    ERROR      0.0000      0.0000   0.0000     1078.0000\n\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 MULTI_CLASSIFICATION Confusion Matrix \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\n  Label      TP      TN    FP     FN\n \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n  ERROR      21418   152   12     178\n  backward   21198   290   148    124\n  bed        21196   236   194    134\n  bird       21172   196   200    192\n  cat        21198   292   122    148\n  dog        20654   286   294    526\n  down       20804   414   140    402\n  eight      20420   230   450    660\n  five       21272   170   144    174\n  follow     21388   214   62     96\n  forward    20618   288   342    512\n  four       20818   584   138    220\n  go         21292   128   62     278\n  happy      20660   86    718    296\n  house      21336   212   102    110\n  learn      20788   450   148    374\n  left       21328   178   42     212\n  marvin     20798   340   146    476\n  nine       20922   602   28     208\n  no         20610   304   346    500\n  off        20736   226   232    566\n  on         20880   364   82     434\n  one        20802   368   166    424\n  right      20428   208   520    604\n  seven      20620   68    716    356\n  sheila     19948   86    1024   702\n  six        20410   212   528    610\n  stop       20536   500   414    310\n  three      20386   112   988    274\n  tree       20796   618   116    230\n  two        20882   626   28     224\n  up         21412   166   18     164\n  visual     21330   252   18     160\n  wow        20744   318   178    520\n  yes        21048   294   126    292\n  zero       20682   0     1078   0\n")),(0,o.kt)("blockquote",null,(0,o.kt)("p",{parentName:"blockquote"},"\ud83d\udca1 Docker is required to start as a demon service on the machine")),(0,o.kt)("p",null,"Congratulations, we have nearly finished the whole example! From now on, we can update the training method, get a new model, build a new SWMP and evaluate our model from time to time."),(0,o.kt)("h3",{id:"evaluate-the-model-on-a-cloud-instance"},"Evaluate the model on a cloud instance"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("strong",{parentName:"li"},"Log in to a cloud instance"))),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-shell"},"(audio_pytorch) $ swcli instance login http://console.pre.intra.starwhale.ai --username starwhale --password abcd1234 --alias pre-k8s\n\u200d\ud83c\udf73 login http://console.pre.intra.starwhale.ai successfully!\n")),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("strong",{parentName:"li"},"Copy the model we built before to the cloud instance"))),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-shell"},"(audio_pytorch) $ swcli model copy m5/version/mfstoolehayd cloud://pre-k8s/project/1\n\ud83d\udea7 start to copy local/project/self/model/m5/version/mfstoolehayd -> http://console.pre.intra.starwhale.ai/project/1...\n  \ud83c\udfb3 upload mfstoolehaydeyrvmyzdamzrmzshuma.swmp \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100% 0:00:07 94.0 MB 10.0 MB/s\n\ud83d\udc4f copy done.\n")),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("strong",{parentName:"li"},"Copy the dataset we built before to the cloud instance"))),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-shell"},"(audio_pytorch) $ swcli dataset copy speechcommands/version/gmzgczrqmezd cloud://pre-k8s/project/1\n\ud83d\udea7 start to copy local/project/self/dataset/speechcommands/version/gmzgczrqmezd -> http://console.pre.intra.starwhale.ai/project/1...\n  \u2b06 _manifest.yaml         \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100% 0:00:00 4.3 kB  ?\n  \u2b06 data_ubyte_0.swds_bin  \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100% 0:00:05 72.3 MB 9.6 MB/s\n  \u2b06 data_ubyte_1.swds_bin  \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100% 0:00:13 72.8 MB 9.7 MB/s\n  \u2b06 data_ubyte_2.swds_bin  \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100% 0:00:21 72.6 MB 9.7 MB/s\n  \u2b06 data_ubyte_3.swds_bin  \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100% 0:00:29 72.6 MB 9.7 MB/s\n  \u2b06 data_ubyte_4.swds_bin  \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100% 0:00:37 73.0 MB 9.8 MB/s\n  \u2b06 data_ubyte_5.swds_bin  \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100% 0:00:45 72.8 MB 9.4 MB/s\n  \u2b06 data_ubyte_6.swds_bin  \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100% 0:00:53 72.8 MB 9.6 MB/s\n  \u2b06 data_ubyte_7.swds_bin  \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100% 0:01:01 72.7 MB 9.7 MB/s\n  \u2b06 data_ubyte_8.swds_bin  \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100% 0:01:09 72.5 MB 9.7 MB/s\n  \u2b06 data_ubyte_9.swds_bin  \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100% 0:01:13 32.5 MB 6.9 MB/s\n  \u2b06 index.jsonl            \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100% 0:01:15 15.2 kB ?\n  \u2b06 label_ubyte_0.swds_bin \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100% 0:01:15 36.6 kB ?\n  \u2b06 label_ubyte_1.swds_bin \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100% 0:01:15 36.6 kB ?\n  \u2b06 label_ubyte_2.swds_bin \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100% 0:01:15 36.6 kB ?\n  \u2b06 label_ubyte_3.swds_bin \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100% 0:01:15 36.6 kB ?\n  \u2b06 label_ubyte_4.swds_bin \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100% 0:01:15 36.6 kB ?\n  \u2b06 label_ubyte_5.swds_bin \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100% 0:01:15 36.6 kB ?\n  \u2b06 label_ubyte_6.swds_bin \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100% 0:01:15 36.6 kB ?\n  \u2b06 label_ubyte_7.swds_bin \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100% 0:01:15 36.6 kB ?\n  \u2b06 label_ubyte_8.swds_bin \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100% 0:01:15 36.6 kB ?\n  \u2b06 label_ubyte_9.swds_bin \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100% 0:01:15 16.3 kB ?\n  \u2b06 archive.swds_meta      \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100% 0:01:23 93.9 MB 10.0 MB/s\n\ud83d\udc4f copy done\n")),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("strong",{parentName:"li"},"Copy the runtime we built before to the cloud instance"))),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-shell"},"(audio_pytorch) $ swcli runtime copy audio_pytorch/version/ga2wkmbwmizw cloud://pre-k8s/project/1\n\ud83d\udea7 start to copy local/project/self/runtime/audio_pytorch/version/ga2wkmbwmizw -> http://console.pre.intra.starwhale.ai/project/1...\n  \ud83c\udfb3 upload ga2wkmbwmizwkn3bmuytsmjunv3dc3q.swrt \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100% 0:00:00 20.5 kB ?\n\ud83d\udc4f copy done.\n")),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("strong",{parentName:"li"},"Go to the console and create one job"))))}u.isMDXComponent=!0}}]);